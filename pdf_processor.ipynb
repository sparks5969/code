{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1. import the necessary libraries and download tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sining/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This section is for importing the necessary libraries\n",
    "import os\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import tiktoken\n",
    "import nltk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# Download the NLTK data required for sentence tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2. define the functions to process original pdf\n",
    "* function 1. convert pdf to text\n",
    "* function 2. split the text string to batches (and save as json files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path_to_file, file_name):\n",
    "    \"\"\"\n",
    "    this function is designed to convert pdf to text\n",
    "    :param path_to_file: the path to the pdf file\n",
    "    :param file_name: the name of the pdf file\n",
    "    :return: the text of the pdf\n",
    "    \"\"\"\n",
    "    # open the pdf file\n",
    "    pdf_file = open(path_to_file + file_name, 'rb')\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    # Get the number of pages\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Extract text from each page\n",
    "    text = ''\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    # make sure the text start with letter, delete /n or empty space at the beginning\n",
    "    while text[0].isalpha() == False:\n",
    "        text = text[1:]\n",
    "\n",
    "    # Close the PDF file\n",
    "    pdf_file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_batches(text, max_tokens=50, model='gpt2'):\n",
    "    \"\"\"\n",
    "    this function is designed to split the text into batches\n",
    "    :param text: the text to be split\n",
    "    :param max_tokens: the maximum number of tokens per batch\n",
    "    :return: a list of batches\n",
    "    \"\"\"\n",
    "    # Load the encoding for the language model\n",
    "    encoding = tiktoken.get_encoding(model)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Initialize an empty list to store the batches\n",
    "    batches = []\n",
    "    \n",
    "    # Initialize the current batch with an empty list\n",
    "    current_batch = []\n",
    "    current_batch_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Encode the sentence into tokens\n",
    "        sentence_tokens = encoding.encode(sentence)\n",
    "        \n",
    "        # If adding the sentence to the current batch would make it too long,\n",
    "        # add the current batch to the list of batches and start a new batch\n",
    "        if len(current_batch_tokens) + len(sentence_tokens) > max_tokens:\n",
    "            batches.append(encoding.decode(current_batch_tokens))\n",
    "            current_batch = [sentence]\n",
    "            current_batch_tokens = sentence_tokens\n",
    "        else:\n",
    "            # Otherwise, add the sentence to the current batch\n",
    "            current_batch.append(sentence)\n",
    "            current_batch_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    # Add the last batch to the list of batches\n",
    "    if current_batch_tokens:\n",
    "        batches.append(encoding.decode(current_batch_tokens))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3. execute the functions to process the pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing essay4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:01,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Processed essay4.pdf\n",
      "Processing essay5.pdf\n",
      "Successfully Processed essay5.pdf\n",
      "Processing essay2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Processed essay2.pdf\n",
      "Processing essay3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Processed essay3.pdf\n",
      "Processing essay1.pdf\n",
      "Successfully Processed essay1.pdf\n",
      "All DONE!====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # specify the local path to the pdf files\n",
    "    folder_path = '/Users/sining/Library/CloudStorage/GoogleDrive-sxw924@case.edu/My Drive/Research/pedagogical research/GenerativeAI for teaching and learning/GenAI in Essay Writing/NudgeEssayResearch/clean_file/test/'\n",
    "\n",
    "    # get a list of the pdf files in the directory\n",
    "    pdf_files = os.listdir(folder_path)\n",
    "\n",
    "\n",
    "    # process the pdf files automatically. If the file not found, catch the exception, skip the file and print the error message\n",
    "    error_file = []\n",
    "    for file in tqdm(pdf_files):\n",
    "        print(f'Processing {file}')\n",
    "        try:\n",
    "            text = pdf_to_text(folder_path, file)\n",
    "            batches = split_text_into_batches(text)\n",
    "            batches = [str(batch) for batch in batches]      \n",
    "            # remove the .pdf from the file name\n",
    "            file_name = file.replace('.pdf', '')\n",
    "            # export the batches to a JSON file, use the file name as the key, and the batches as the value\n",
    "            json.dump(batches, open('rawtext/' + file_name + '.json', 'w'))\n",
    "            print(f'Successfully Processed {file}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file}: {e}')\n",
    "            error_file.append(file) \n",
    "    \n",
    "    print(\"All DONE!====================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
